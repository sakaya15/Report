# 情報理論
## 自己情報量・シャノンエントロピーの定義
### 自己情報量
事象$A$が起こる確率が$P(A)$の時,自己情報量は以下のように定義する.対数の底が2の時単位はビット、対数の底がネイピア数eの時単位はネットとする.

$$
I(A)=-\log(P(A))
$$

### シャノンエントロピー
離散確率変数$X$において$X=x$となる確率が$p(x)$で与えられている時,シャノンエントロピーは以下のように定義する.(自己情報量の期待値)

$$
H(X)=E(I(X))=-E(\log(p(x))=-\Sigma p(x)\log(p(x))
$$

## KLダイバージェンス・交差エントロピー
### KLダイバージェンス
2つの確率分布p(x)とq(x)のKLダイバージェンスは以下のように定義する.(確率分布p(x)とq(x)の違いを表す)

$$
D_{KL} (p||q)=E \left(\frac{p(x)}{q(x)}\right)
$$

### 交差エントロピー
交差エントロピーは以下のように定義する.(qについての自己情報量をpの分布で平均している)


$$
\begin{equation*}
\begin{split}
H(p,q)
&=H(p)+D_{KL} (p||q)\\
&=-E\log q(x)\\
&=-\Sigma_{x}p(x)\log q(x)
\end{split}
\end{equation*}
$$