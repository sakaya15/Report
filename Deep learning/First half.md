# ニューラルネットワークの全体像

* 深層学習で行おうとしていることは、明示的なプログラムの代わりに多数の中間層を持つニューラルネットワークを用いて、入力値から目的とする出力値に変換する数学モデルを構築すること

* ニューラルネットワークは最初の層を入力層、中間の層を中間層、最後の層を出力層と呼ぶ

* 入力層から中間層・中間層から出力層への伝達時は、重み$W$とバイアス$b$がかかり、最終的には最適な重みとバイアスを発見することが学習の目的となる

確認テスト
* ディープラーニングは、結局何をやろうとしているか2行以内で述べよ  
明示的なプログラムの代わりに多数の中間層を持つニューラルネットワークを用いて、入力値から目的とする出力値に変換する数学モデルを構築すること

* どの値の最適化が最終目的か  
重み、バイアス

# 1-1.　入力層〜中間層

* 入力層で受け取った情報$x$は、重み$W$とバイアス$b$によって以下の様に示される

$$
W=
\begin{pmatrix}
   w_{1} \\
   \vdots\\
   w_{l} 
\end{pmatrix}
$$

$$
x=
\begin{pmatrix}
   x_{1} \\
   \vdots\\
   x_{l} 
\end{pmatrix}
$$
$$
u=\bm{W^{T}x}+\bm{b}
$$

# 1-2.活性化関数

* 活性化関数とは、ニューラルネットワークにおいて、次の層への出力の大きさを決める非線形の関数のこと

* 中間層用の活性化関数には、ステップ関数・シグモイド関数・ReLU関数等がある

* 出力層用の活性化関数には、ソフトマックス関数・恒等写像・シグモイド関数等がある

ステップ関数

$$
\begin{equation*}
f(x)
=
\begin{cases}
1　& \text{($x\geq0)$} \\
0  & \text{($x<0)$} 

\end{cases}
\end{equation*}
$$

シグモイド関数
$$
f(x)=\frac{1}{1+e^{-x}}
$$


ReLU関数
$$
\begin{equation*}
f(x)
=
\begin{cases}
x　& \text{($x>0)$} \\
0  & \text{($x\leq0)$} 

\end{cases}
\end{equation*}
$$

確認テスト
* 線形と非線形の違いを簡易に説明せよ  
線形な関数は加法性$（f(x+y)=f(x)+f(y)f(x+y)=f(x)+f(y)）$と斉次性$（f(kx)=kf(x)f(kx)=kf(x)）$を満たすが、非線形な関数は満たさない

# 1-3. 出力層

* 出力層の役割は、各クラスに分類される確率を出力すること

* 出力層用の活性化関数には、ソフトマックス関数・恒等写像・シグモイド関数等がある


ソフトマックス関数
$$
f(i,x)=\frac{e^{x_{i}}}{\sum^{K}_{k=1}e^{x_{k}}}
$$
恒等写像
$$
f(x)=x
$$
## 誤差関数

* 出力層より得られた学習結果と訓練データより得られる正解を、誤差関数を使って検証することで、ニューラルネットワークによりどの程度よい結果が得られたかを判断することができる

* 誤差関数には、二乗和誤差と交差エントロピー誤差が用いられる

### 二乗和誤差
$$
E_{n}(w)=\frac{1}{2}\sum^{I}_{i=1}(y_{n}-d_{n})^{2}
$$
### 交差エントロピー誤差

$$
E_{n}(w)=-\sum^{I}_{i=1}d_{i}\log y_{i}
$$






# 1-4. 勾配降下法

* 深層学習の目的は、学習を通して誤差を最小にするネットワークを作成すること.つまり、誤差を最小化するようなパラメータ見つけること.

* パラメータを最適化するための手法の一つが勾配降下法

* 勾配降下法は以下の式で表せる
$$
w^{(t+1)}=w^{(t)}-\varepsilon \nabla	E  
$$

$$
\nabla	E=\frac{\partial E}{\partial w}=(\frac{\partial E}{\partial w_{1}}\cdots \frac{\partial E}{\partial w_{M}})

$$
* 学習率が大き過ぎると最小値にいつまでも辿りつかず発散する

* 学習率が小さすぎると収束するまでに時間がかかる

* 学習率の決定、収束性向上のためのアルゴリズムとしてMomentum,AdaGrad,Adadelta,Adam等がある

## 4-2. 確率的勾配降下法

・ランダムに抽出したサンプルを用いて誤差を求める方法

・勾配降下法と比較して、データが冗長な場合の計算コストの軽減、望まない局所極小解に収束するリスクの軽減、オンライン学習ができるなどのメリットがある

・オンライン学習とは、学習データが入ってくるたびに都度パラメータを更新し、学習を進めていく方法である

4-3. ミニバッチ勾配降下法

・オンライン学習の特徴をうまくバッチ学習で使えるようにした手法

・ランダムに分割したデータの集合（ミニバッチ）DtDtに属するサンプルの平均誤差

f(x)

・確率的勾配降下法のメリットを損なわず、計算機の計算資源を有効利用できる、すなわち、CPUを利用したスレッド並列化やGPUを利用したSIMD並列化を実現できるのがミニバッチ勾配降下法のメリットである

# 1-5. 誤差逆伝播法

・誤差逆伝播法とは、算出された誤差を出力層側から順に微分し、前の層へと伝播させる手法

・最小限の計算で各パラメータにおける微分値を解析的に計算することができる

# 2-1.勾配消失問題

•勾配消失問題とは、誤差逆伝播が下位層に進んでいくにつれて、勾配がどんどん緩やかになっていき、下位層のパラメータがほとんど変わらず最適値に収束しなくなることを指す

・誤差逆伝播の特徴として、下位層にいくにつれて、微分の回数が多くなる
微分値が1より小さい値をとる場合、計算結果はほとんど0に近い状態となり、勾配消失が起こる

・シグモイド関数の微分値は、最大0.25であるため、中間層が多くなるにつれて勾配消失が起こりやすくなる

# 6-2. 勾配消失の対策

活性化関数の選択

重みの初期値設定

バッチ正規化

# 2-2.学習率最適化手法

# 2-3. 過学習とは
・過学習とは、学習が訓練データにフィットし過ぎてしまい、訓練誤差が減少しているのにも関わらず、テスト誤差が増加してしまう現象を指す

## 過学習の対策手法

・正則化手法（ネットワークの自由度を制約すること）を利用して過学習を抑制する

•L1正則化をラッソ回帰、L2正則化をリッジ回帰を呼ぶ

・ドロップアウトという、ランダムにノードを削除して学習させることで過学習を抑止する

・データ量を変化させずに、異なるモデルを学習させていると解釈できる

# 2-4. CNN

f(x)

# 2-5.最新のCNN

# 10-1. AlexNetのモデル説明

・AlexNetとは2012年に開かれた画像認識コンペティション2位に大差をつけて優勝したモデルである

・AlexNetの登場で、ディープラーニングが大きく注目を集めた

・LeNetが世に出てから20年近くが経過して、AlexNetが発表された

・AlexNetは、5層の畳み込み層およびプーリング層など、それに続く3層の全結合層から構成される

・活性化関数にReLu関数を用いる点、LRNという局所的正規化を行う層を用いる点、ドロップアウトを使用する点がLeNetとの違いである


