1-1.　ニューラルネットワークの全体像

・深層学習で行おうとしていることは、明示的なプログラムの代わりに多数の中間層を持つニューラルネットワークを用いて、入力値から目的とする出力値に変換する数学モデルを構築することである

・ニューラルネットワークは下図のように表せ、一番左の層を入力層、中間の層を中間層、一番右の層を出力層と呼ぶ

•入力層→中間層、中間層→出力層への伝達時は、重み（W）とバイアス（b）がかかり、最終的には最適な重みとバイアスを発見することが学習の目的となる

1-2.　入力層〜中間層

・入力層とは、ニューラルネットワークに何かしらの数値情報を入力するための層である

・中間層とは、入力層が受け取った数値情報に重みとバイアスを加えた情報を受け取り、活性化関数を通して情報を出力層に伝達する層である

・入力層で受け取った情報xは、重みWとバイアスbによって以下の様に編集される

$$
W=
\begin{pmatrix}
   w_{1} \\
   \vdots\\
   w_{l} 
\end{pmatrix}
$$

$$
x=
\begin{pmatrix}
   x_{1} \\
   \vdots\\
   x_{l} 
\end{pmatrix}
$$
$$
u=\bm{Wx}+\bm{b}
$$

活性化関数

・活性化関数とは、ニューラルネットワークにおいて、次の層への出力の大きさを決める非線形の関数のことである


・入力値の値によって、次の層への信号のON/OFFや強弱を定める働きをもつ

・活性化関数は、非線形な関数であるが、線形な関数は加法性（f(x+y)=f(x)+f(y)f(x+y)=f(x)+f(y)）と斉次性（f(kx)=kf(x)f(kx)=kf(x)）を満たすが、非線形な関数はこれらを満たさない


・中間層用の活性化関数には、ReLU関数、シグモイド関数、ステップ関数がある


・出力層用の活性化関数には、ソフトマックス関数、恒等写像、シグモイド関数がある

ステップ関数



f(x)



シグモイド関数

f(x)



ReLU関数

f(x)



3-1. 出力層

・出力層の役割は、各クラスに分類される確率（私たちが欲しい情報）を出すことである

・出力層と中間層の違いとして、中間層が閾値の前後で信号の強弱を調整する役割を担うのに対して、出力層では、信号の大きさ（比率）はそのままに変換を行う点が挙げられる

・出力層で用いられる活性化関数には、恒等写像、シグモイド関数、ソフトマックス関数がある

ソフトマックス関数

f(x)

3-2. 誤差関数

・出力層より得られた学習結果と訓練データより得られる正解を、誤差関数を使って検証することで、ニューラルネットワークによりどの程度よい結果が得られたかを判断することができる

・誤差関数には、二乗和誤差と交差エントロピー誤差が使われる

二乗和誤差

f(x)

交差エントロピー誤差

f(x)

4-1. 勾配降下法

・深層学習の目的は、学習を通して誤差を最小にするネットワークを作成すること、すなわち、誤差を最小化するようなパラメータを発見することである

・パラメータを最適化するための手法のひとつが勾配降下法である

・勾配降下法は以下の式で表せる
f(x)

・学習率が大きすぎると最小値にいつまでも辿りつかず発散してしまう

・学習率が小さすぎると収束するまでに時間がかかってしまう


・学習率の決定、収束性向上のためのアルゴリズムには、Momentum,AdaGrad,Adadelta,Adamなどがある

・勾配降下法は以下のように実装できる

4-2. 確率的勾配降下法

・ランダムに抽出したサンプルを用いて誤差を求める方法

・勾配降下法と比較して、データが冗長な場合の計算コストの軽減、望まない局所極小解に収束するリスクの軽減、オンライン学習ができるなどのメリットがある

・オンライン学習とは、学習データが入ってくるたびに都度パラメータを更新し、学習を進めていく方法である

4-3. ミニバッチ勾配降下法

・オンライン学習の特徴をうまくバッチ学習で使えるようにした手法

・ランダムに分割したデータの集合（ミニバッチ）DtDtに属するサンプルの平均誤差

f(x)

・確率的勾配降下法のメリットを損なわず、計算機の計算資源を有効利用できる、すなわち、CPUを利用したスレッド並列化やGPUを利用したSIMD並列化を実現できるのがミニバッチ勾配降下法のメリットである

5-1. 誤差逆伝播法とは

・誤差逆伝播法とは、算出された誤差を出力層側から順に微分し、前の層へと伝播させる手法

・最小限の計算で各パラメータにおける微分値を解析的に計算することができる

6-1. 勾配消失問題とは

•勾配消失問題とは、誤差逆伝播が下位層に進んでいくにつれて、勾配がどんどん緩やかになっていき、下位層のパラメータがほとんど変わらず最適値に収束しなくなることを指す

・誤差逆伝播の特徴として、下位層にいくにつれて、微分の回数が多くなる
微分値が1より小さい値をとる場合、計算結果はほとんど0に近い状態となり、勾配消失が起こる

・シグモイド関数の微分値は、最大0.25であるため、中間層が多くなるにつれて勾配消失が起こりやすくなる

6-2. 勾配消失の対策

活性化関数の選択

重みの初期値設定

バッチ正規化

学習率最適化手法

7-1. 学習率と学習効率

7-2. 学習率の決め方

7-3-1. モメンタム
7-3-2. AdaGrad
7-3-3. RMSProp
7-3-4. Adam

8-1. 過学習とは
・過学習とは、学習が訓練データにフィットし過ぎてしまい、訓練誤差が減少しているのにも関わらず、テスト誤差が増加してしまう現象を指す

8-2. 過学習の対策手法

・正則化手法（ネットワークの自由度を制約すること）を利用して過学習を抑制する

•L1正則化をラッソ回帰、L2正則化をリッジ回帰を呼ぶ

・ドロップアウトという、ランダムにノードを削除して学習させることで過学習を抑止する

・データ量を変化させずに、異なるモデルを学習させていると解釈できる

9-1. CNN

f(x)

最新のCNN

10-1. AlexNetのモデル説明

・AlexNetとは2012年に開かれた画像認識コンペティション2位に大差をつけて優勝したモデルである

・AlexNetの登場で、ディープラーニングが大きく注目を集めた

・LeNetが世に出てから20年近くが経過して、AlexNetが発表された

・AlexNetは、5層の畳み込み層およびプーリング層など、それに続く3層の全結合層から構成される

・活性化関数にReLu関数を用いる点、LRNという局所的正規化を行う層を用いる点、ドロップアウトを使用する点がLeNetとの違いである


