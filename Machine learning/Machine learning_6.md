# サポートベクターマシーン
教師あり学習の分類における数理モデル.
# サポートベクターマシーン（理論）
## 2クラス分類
入力データ$\bm{x}$対してラベルy(1or-1)を出力する分類方法.

### 決定関数
一般的に2クラス分類において、特徴ベクトル$\bm{x}$がどちらのクラスに属するか判定するために用いられる関数$f(\bm{x})$を決定関数といい、よく用いられる.$\bm{w}$はパラメータ、$b$はバイアス.※今回は線形の場合を考える
$$
f(\bm{x})=\bm{w}^{T}\bm{x}+b
$$

$$
\begin{equation*}
y=
sgnf(\bm{x})
=

\begin{cases}
1　& \text{($f(\bm{x})>0)$} \\
-1  & \text{($f(\bm{x})<0)$} 

\end{cases}
\end{equation*}
$$

### 分類境界
特徴ベクトルを分ける境界線を分類境界という.

## 線形サポートベクトル分類(ハードマージン→分類可能)
特徴ベクトルとラベルのセットを訓練データと呼ぶ.
$$
(\bm{x}_{i},y_{i}) \ (i=1,2,\cdots,n)
$$

分離可能な場合
$$
y_{i}f(\bm{x}_{i})>0(i=1,2,\cdots,n)
$$

※分離不可能な場合
$$
y_{i}f(\bm{x}_{i})<0(i=1,2,\cdots,n)
$$

$$
f(\bm{x}_{i})=0(i=1,2,\cdots,n)
$$
を考える.

分類境界を挟んで2つのクラスがどれくらい離れているかをマージンと呼ぶ.
SVMはマージンが大きいものが良い分類境界となる.この考え方をマージン最大化と呼ぶ.
$$
\frac{|f(\bm{x_{i}})|}{||\bm{w}||}=\frac{y_{i}[\bm{w}^{T}\bm{x}_{i}+b]}{||\bm{w}||}
$$
絶対値の定義から、
$$
\begin{equation*}
|\bm{w}^{T}\bm{x}_{i}+b|
=
\begin{cases}
\bm{w}^{T}\bm{x}_{i}+b　& \text{($\bm{w}^{T}\bm{x}_{i}+b\geq0)$} \\
-[\bm{w}^{T}\bm{x}_{i}+b]  & \text{($\bm{w}^{T}\bm{x}_{i}+b<0)$} 

\end{cases}
\end{equation*}
$$
訓練データのラベル$y_{i}とf(\bm{x})$の符号が全てのデータに対して一致している.
つまり、分類境界$f(\bm{x})=0$と分類境界から最も近くにあるデータ$x_{i}$との距離は次のように表現できる.
$$
\underset{i}{min}\frac{y_{i}[\bm{w}^{T}\bm{x}_{i}+b]}{||\bm{w}||}=\frac{M(\bm{w},b)}{||\bm{w}||}
$$
マージンを最大化することは、これを最大化することと等価だから、以下を求めれば良い.
$$
\underset{i}{max}[\underset{i}{min}\frac{y_{i}[\bm{w}^{T}\bm{x}_{i}+b]}{||\bm{w}||}]=\underset{w,b}{max}\frac{M(\bm{w},b)}{||\bm{w}||}
$$
## 線形サポートベクトル分類(ソフトマージン→分類不可能)

## SVMにおける双対表現
ハードマージンの場合とソフトマージンの場合を見てきた.これの最適化を主問題という.しかし、この主問題と等価な双対問題が主に以下のメリットがあり用いられている.

* 主問題と比べて、双対問題の方が変数を少なくできる.
* 分類境界の非線形化を考える上で、双対問題の形式(双対形式)の方が有利となる.

## 双対問題の導出

## 主問題と双対問題の関係

## カーネルを用いた非線形分離への拡張
特徴ベクトルを非線形変換し、その空間で線形分類を行う「カーネルトリック」という手法を用いれば、非線形分離を行うことが可能となる.
# サポートベクターマシーン（実装）